{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d8e3c0-f9f2-4888-b276-2d2f802a61fe",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression with KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a77199e6-10ec-4d39-8896-cab825f63114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5246dc30-705a-422b-96dc-7d86fb4cba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic regression class\n",
    "\n",
    "class logisticRegressionClf:\n",
    "    \n",
    "    def __init__(self, eta = 0.1, nitermax = 50):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.nitermax = nitermax\n",
    "        self.w = None\n",
    "        self.marginmax = 0\n",
    "        \n",
    "        return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Size of the X matrix - m rows and n columns (m x n)\n",
    "        m, n = X.shape\n",
    "\n",
    "        # Initializing the weights - Each feature has a weight\n",
    "        w = np.zeros(n)\n",
    "        self.marginmax = 0\n",
    "\n",
    "        # Iterating\n",
    "\n",
    "        for t in range(self.nitermax):\n",
    "            for i in range(m):\n",
    "                xymargin = y[i]*np.dot(w, X[i])\n",
    "                if xymargin > self.marginmax:\n",
    "                    self.marginmax = xymargin\n",
    "                    \n",
    "                philog = 1/(1 + np.exp(-(-xymargin)))\n",
    "                deltaJ = -philog * y[i] * X[i]\n",
    "                w = w - self.eta * deltaJ # Updating weights\n",
    "        self.w = w\n",
    "        \n",
    "        return \n",
    "    \n",
    "    \n",
    "    def predict(self, X, y = None):\n",
    "        \n",
    "        #print(self.w)\n",
    "        \n",
    "        xw = np.dot(X,self.w)\n",
    "        \n",
    "        # Predicting + 1 probability\n",
    "        Pyplus = 1/(1+np.exp(-xw))\n",
    "        # Predicting -1 probability\n",
    "        Pyminus = 1/(1+np.exp(xw))\n",
    "        \n",
    "        # Labels with greater probabilities\n",
    "        y = 2*(Pyplus > Pyminus) -1\n",
    "        \n",
    "        return(y)\n",
    "    \n",
    "    def main():\n",
    "        \n",
    "        # Data\n",
    "        from sklearn.datasets import load_breast_cancer\n",
    "        X, y = load_breast_cancer(return_X_y=True)\n",
    "        # Convert {0,1} output into {-1,+1}\n",
    "        y = 2*y - 1  \n",
    "        \n",
    "        # Scaling\n",
    "        #scaler = StandardScaler()\n",
    "        #X = scaler.transform(X) # Scaling with L2\n",
    "        \n",
    "        mdata, ndim = X.shape\n",
    "        \n",
    "        nitermax = 50  # maximum iteration\n",
    "        eta = 0.1 # learning speed \n",
    "\n",
    "        nfold = 5 # number of folds \n",
    "\n",
    "        ## Split the data into 5-folds\n",
    "        cselection = KFold(n_splits=nfold, random_state=None, shuffle=False)\n",
    "\n",
    "        clogreg = logisticRegressionClf()\n",
    "        \n",
    "        X /= np.outer(np.ones(X.shape[0]), np.max(np.abs(X),0))\n",
    "\n",
    "        xf1 = np.zeros(nfold)\n",
    "\n",
    "        n = 0\n",
    "        \n",
    "        for index_train, index_test in cselection.split(X):\n",
    "            Xtrain = X[index_train]\n",
    "            ytrain = y[index_train]\n",
    "            Xtest = X[index_test]\n",
    "            ytest = y[index_test]\n",
    "\n",
    "            clogreg.fit(Xtrain, ytrain)\n",
    "            yprediction = clogreg.predict(Xtest)\n",
    "                        \n",
    "            f1 = f1_score(yprediction, ytest)\n",
    "            \n",
    "            print(\"F1 score is:\", f1)\n",
    "            xf1[n] = f1\n",
    "            n += 1\n",
    "            \n",
    "        print(\"The average F1: \", np.mean(xf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "87e0adea-32f2-4e79-96db-04777f79ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.8823529411764706\n",
      "F1 score is: 0.9285714285714286\n",
      "F1 score is: 0.9605263157894737\n",
      "F1 score is: 0.9941520467836257\n",
      "F1 score is: 0.983050847457627\n",
      "The average F1:  0.949730715955725\n"
     ]
    }
   ],
   "source": [
    "clf = logisticRegressionClf\n",
    "\n",
    "clf.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
